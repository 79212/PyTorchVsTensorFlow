{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327537</th>\n",
       "      <td>1</td>\n",
       "      <td>@sandieb321  Nice to have some time to yoursel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9581</th>\n",
       "      <td>0</td>\n",
       "      <td>@The_Tyree I miss the homie Remy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345433</th>\n",
       "      <td>1</td>\n",
       "      <td>@FirstGentleman Okay we can agree on that...to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38552</th>\n",
       "      <td>0</td>\n",
       "      <td>Good morning tweeples... I hope it is as brigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219419</th>\n",
       "      <td>1</td>\n",
       "      <td>@MajorDodson awwww...how sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170732</th>\n",
       "      <td>0</td>\n",
       "      <td>@Freebies4Mom will they have your video availa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84703</th>\n",
       "      <td>0</td>\n",
       "      <td>School Tmz  About to watch Rove ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104248</th>\n",
       "      <td>0</td>\n",
       "      <td>lost  have to go on toll road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109078</th>\n",
       "      <td>0</td>\n",
       "      <td>@TraceyDukes hate cats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337607</th>\n",
       "      <td>1</td>\n",
       "      <td>@loretto1 i know! were like crazy smart to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388633</th>\n",
       "      <td>1</td>\n",
       "      <td>@simonmayo Well if you're a DJ now, here's an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258226</th>\n",
       "      <td>1</td>\n",
       "      <td>@KellyKapoor  Now that's smart!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130455</th>\n",
       "      <td>0</td>\n",
       "      <td>@iheartlambert me too  noykalahlah XOXOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324871</th>\n",
       "      <td>1</td>\n",
       "      <td>@mileycyrus aww  i bought my mom a locket that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196258</th>\n",
       "      <td>0</td>\n",
       "      <td>@kippras Please do turn them off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376327</th>\n",
       "      <td>1</td>\n",
       "      <td>twitter EVER for the next two weeks! i'll only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161899</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't think I am going to get to see any of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355108</th>\n",
       "      <td>1</td>\n",
       "      <td>@lisanti when you get time if you could send m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190091</th>\n",
       "      <td>0</td>\n",
       "      <td>I can't believe today is the last Tonight Show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308054</th>\n",
       "      <td>1</td>\n",
       "      <td>@Perpetual_Kid May the 4th be with you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiments                                             tweets\n",
       "327537           1  @sandieb321  Nice to have some time to yoursel...\n",
       "9581             0                 @The_Tyree I miss the homie Remy! \n",
       "345433           1  @FirstGentleman Okay we can agree on that...to...\n",
       "38552            0  Good morning tweeples... I hope it is as brigh...\n",
       "219419           1                    @MajorDodson awwww...how sweet \n",
       "170732           0  @Freebies4Mom will they have your video availa...\n",
       "84703            0                 School Tmz  About to watch Rove ;)\n",
       "104248           0                      lost  have to go on toll road\n",
       "109078           0                            @TraceyDukes hate cats \n",
       "337607           1  @loretto1 i know! were like crazy smart to be ...\n",
       "388633           1  @simonmayo Well if you're a DJ now, here's an ...\n",
       "258226           1                   @KellyKapoor  Now that's smart! \n",
       "130455           0           @iheartlambert me too  noykalahlah XOXOX\n",
       "324871           1  @mileycyrus aww  i bought my mom a locket that...\n",
       "196258           0                @kippras Please do turn them off.  \n",
       "376327           1  twitter EVER for the next two weeks! i'll only...\n",
       "161899           0  I don't think I am going to get to see any of ...\n",
       "355108           1  @lisanti when you get time if you could send m...\n",
       "190091           0  I can't believe today is the last Tonight Show...\n",
       "308054           1            @Perpetual_Kid May the 4th be with you "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data from csv file\n",
    "\n",
    "data_path = './data/tweets.csv'\n",
    "\n",
    "data = pd.read_csv(data_path, usecols=[0,5], encoding='utf-8', names=['sentiments', 'tweets'])\n",
    "data = data.sample(frac=.10)\n",
    "\n",
    "data.sentiments = [0 if x==0 else 1 for x in data.sentiments]\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting some variables\n",
    "\n",
    "ngram_range = 1\n",
    "max_features = 20000\n",
    "maxlen = 42\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0 & Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiating keras tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token='<00v>')\n",
    "tokenizer.fit_on_texts(data.tweets)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 42)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding the sequences so that they have unoform dimension\n",
    "\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = padded\n",
    "y_data = np.array(data.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 42, 32)            640000    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 24)                32280     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 672,305\n",
      "Trainable params: 672,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# configuring the model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/2\n",
      "32000/32000 - 13s - loss: 0.5436 - accuracy: 0.7160 - val_loss: 0.4717 - val_accuracy: 0.7754\n",
      "Epoch 2/2\n",
      "32000/32000 - 13s - loss: 0.3689 - accuracy: 0.8401 - val_loss: 0.4985 - val_accuracy: 0.7716\n",
      "Wall time: 26.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a7c57c6408>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fitting the model\n",
    "\n",
    "model.fit(x=X_data, y=y_data, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = torch.LongTensor(padded)\n",
    "y_data = torch.FloatTensor(np.array(data.sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tweets, sentiments):\n",
    "        self.tweets = tweets\n",
    "        self.sentiments = sentiments\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = {\"tweets\":self.tweets[index,:], \"sentiments\":self.sentiments[index]}\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.tweets.shape[0]\n",
    "    \n",
    "tweets_dataset = MyDataset(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(tweets_dataset, batch_size=batch_size,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuring the model\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embedding_dim),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(maxlen*batch_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6882041096687317\n",
      "0.6311270594596863\n",
      "0.6742776036262512\n",
      "0.5982884764671326\n",
      "0.6217235326766968\n",
      "0.6711956858634949\n",
      "0.6582696437835693\n",
      "0.5244784355163574\n",
      "0.5245709419250488\n",
      "0.546600341796875\n",
      "0.42615628242492676\n",
      "0.5278566479682922\n",
      "0.5820406079292297\n",
      "0.6533368825912476\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, sample in enumerate(dataloader):\n",
    "        \n",
    "        X = sample[\"tweets\"]\n",
    "        y = sample[\"sentiments\"]\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            print(loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with CNN tbd\n",
    "\n",
    "embedding_dim = 64\n",
    "max_seq_len = 46\n",
    "vocab_size = 20000\n",
    "n_filters = 128\n",
    "kernel_size = 5\n",
    "batch_size = 32\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embedding_dim),\n",
    "    nn.Conv1d(max_seq_len, embedding_dim, kernel_size),\n",
    "    nn.MaxPool1d(46), \n",
    "    nn.Linear(46, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
